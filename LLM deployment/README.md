

# å¤§æ¨¡å‹æœ¬åœ°éƒ¨ç½²ä¸æ€§èƒ½å‹æµ‹å®æˆ˜ (AutoDL + Xinference/vLLM)

æœ¬é¡¹ç›®è®°å½•äº†åœ¨ AutoDL å¹³å°ä¸Šï¼Œåˆ©ç”¨ **4090D (24G)** æ˜¾å¡è¿›è¡Œå¤§æ¨¡å‹ï¼ˆQwen3 ç³»åˆ—ï¼‰æœ¬åœ°éƒ¨ç½²çš„å…¨è¿‡ç¨‹ï¼Œæ¶µç›–äº† **Xinference** å’Œ **vLLM** ä¸¤ç§ä¸»æµåç«¯ï¼Œå¹¶é™„å¸¦äº†åŸºäº **Locust** çš„å¹¶å‘å‹åŠ›æµ‹è¯•åˆ†æã€‚

---

## ğŸ› ï¸ ç¯å¢ƒé…ç½®

### 1. åŸºç¡€ç¯å¢ƒå‡†å¤‡
åœ¨ AutoDL çš„æ•°æ®ç›˜ï¼ˆ`/root/autodl-tmp`ï¼‰ä¸­åˆ›å»ºéš”ç¦»çš„ Conda ç¯å¢ƒï¼š

```bash
cd /root/autodl-tmp

# åˆ›å»ºå¹¶æ¿€æ´»ç¯å¢ƒ
conda create -n llm_deploy python=3.12 -y
conda activate llm_deploy

# å®‰è£…æ¨ç†æ¡†æ¶
pip install "xinference[vllm,transformers]" locust openai

```

### 2. åŠ é€Ÿä¸è·¯å¾„ä¼˜åŒ–

é…ç½®ç¯å¢ƒå˜é‡ä»¥ä½¿ç”¨ **ModelScope (é­”æ­)** é•œåƒåŠ é€Ÿæ¨¡å‹ä¸‹è½½ï¼Œå¹¶æŒ‡å®šæ•°æ®ç›˜å­˜å‚¨è·¯å¾„ï¼š

```bash
# è®¾ç½®æ¨¡å‹å­˜å‚¨è·¯å¾„
export XINFERENCE_HOME=/root/autodl-tmp/xinference
# åˆ‡æ¢æ¨¡å‹æºä¸º ModelScope (å›½å†…è®¿é—®åŠ é€Ÿ)
export XINFERENCE_MODEL_SRC=modelscope

# å†™å…¥é…ç½®æ–‡ä»¶æ°¸ä¹…ç”Ÿæ•ˆ
echo 'export XINFERENCE_HOME=/root/autodl-tmp/xinference' >> ~/.bashrc
echo 'export XINFERENCE_MODEL_SRC=modelscope' >> ~/.bashrc
source ~/.bashrc

```

---

## ğŸš€ éƒ¨ç½²æ–¹æ¡ˆä¸€ï¼šXinference

Xinference æä¾›äº†ä¾¿æ·çš„ GUI ç•Œé¢å’Œå¤šå¼•æ“æ”¯æŒã€‚

### 1. å¯åŠ¨æœåŠ¡

```bash
# å»ºè®®åå°è¿è¡Œå¹¶è®°å½•æ—¥å¿—
nohup xinference-local --host 0.0.0.0 --port 6006 > xinference.log 2>&1 &

```

### 2. æ¨¡å‹é…ç½®è¦ç‚¹

åœ¨ Web ç•Œé¢éƒ¨ç½² `Qwen3-4B-fp8` æ—¶ï¼Œå…³é”®å‚æ•°å»ºè®®å¦‚ä¸‹ï¼š

* **Engine**: é€‰æ‹© `vLLM` ä»¥è·å¾—æœ€ä½³ååã€‚
* **gpu_memory_utilization**: `0.70` (é¢„ç•™éƒ¨åˆ†æ˜¾å­˜ç”¨äº KV Cache åŠ¨æ€å¢é•¿)ã€‚
* **max_model_len**: `2048` æˆ– `4096` (å‡å°æ­¤å€¼å¯æ˜¾è‘—é™ä½æ˜¾å­˜å ç”¨ï¼Œæå‡å¹¶å‘èƒ½åŠ›)ã€‚

---

## ğŸ“ˆ æ€§èƒ½å‹æµ‹åˆ†æ (Locust)

é’ˆå¯¹ `Qwen3-4B-fp8` æ¨¡å‹ï¼Œåœ¨è¾“å‡º Token å›ºå®šä¸º 50 çš„åœºæ™¯ä¸‹è¿›è¡Œå‹åŠ›æµ‹è¯•ï¼š

### å¹¶å‘æ€§èƒ½å¯¹æ¯”è¡¨

| æŒ‡æ ‡ | 50 å¹¶å‘ | 500 é«˜å¹¶å‘ | å˜åŒ–ç»“è®º |
| --- | --- | --- | --- |
| **æ¯ç§’åå (RPS)** | 30.71 | 91.77 | ååé‡éšå¹¶å‘å¤§å¹…å¢é•¿ï¼ŒGPU åˆ©ç”¨ç‡é¥±å’Œ |
| **å¹³å‡å“åº”æ—¶é—´** | 1.18 ç§’ | 3.21 ç§’ | å»¶è¿Ÿå¢åŠ ï¼Œç³»ç»Ÿå¼€å§‹æ’é˜Ÿ |
| **95% åˆ†ä½å»¶è¿Ÿ** | 1.20 ç§’ | 3.70 ç§’ | é«˜å¹¶å‘ä¸‹ç”¨æˆ·ä½“éªŒä¸‹é™çº¦ 2 ç§’ |

> **é•¿æ–‡æœ¬å‹åŠ›ç»“è®º**ï¼šåœ¨è¾“å‡º 512 Token ä¸”å¹¶å‘ä¸º 500 æ—¶ï¼Œé¦–å­—å»¶è¿Ÿ (TTFT) é£™å‡è‡³çº¦ 15sã€‚è™½ç„¶æ€»ååèƒ½è¾¾åˆ°çº¦ **7500 Token/s**ï¼Œä½†æé•¿çš„æ’é˜Ÿæ—¶é—´æ„å‘³ç€å•å¡ 4090D éš¾ä»¥æ”¯æ’‘ 500 ä¸ªç”¨æˆ·çš„é•¿æ–‡æœ¬å®æ—¶å¯¹è¯ã€‚

---

## âš¡ éƒ¨ç½²æ–¹æ¡ˆäºŒï¼švLLM åŸç”Ÿéƒ¨ç½²

vLLM æ˜¯ç›®å‰æœ€é«˜æ•ˆçš„æ¨ç†å¼•æ“ä¹‹ä¸€ï¼Œæ”¯æŒ BF16 ç²¾åº¦å’Œ DeepSeek å¼çš„æ¨ç†è§£æã€‚

### 1. å¯åŠ¨å‘½ä»¤

```bash
vllm serve /root/autodl-tmp/model/Qwen/Qwen3-4B-Instruct-2507 \
    --served-model-name Qwen3-4B \
    --max_model_len 1024 \
    --reasoning-parser deepseek_r1 \
    --host 0.0.0.0 \
    --port 6006

```

### 2. Python å®¢æˆ·ç«¯è°ƒç”¨ (OpenAI SDK)

```python
from openai import OpenAI

# ä½¿ç”¨ AutoDL æä¾›çš„å…¬ç½‘æ˜ å°„åœ°å€
client = OpenAI(
    api_key="EMPTY", 
    base_url="https://<your-autodl-url>:8443/v1"
)

response = client.chat.completions.create(
    model="Qwen3-4B",
    messages=[{"role": "user", "content": "ä½ å¥½ï¼Œè¯·å¼€å§‹æ¨ç†"}]
)

print(f"Content: {response.choices[0].message.content}")
# å¦‚æœä½¿ç”¨äº†æ¨ç†æ¨¡å‹ï¼Œå¯æ‰“å°æ€è€ƒè¿‡ç¨‹
print(f"Reasoning: {response.choices[0].message.reasoning_content}")

```

---

## ğŸ’¡ æ€»ç»“ä¸å»ºè®®

1. **æ˜¾å­˜ç®¡ç†**ï¼š4090D (24G) åœ¨éƒ¨ç½² 4B æ¨¡å‹æ—¶éå¸¸ä»å®¹ï¼Œä½†å¹¶å‘å¢åŠ æ—¶ KV Cache æ˜¯ä¸»è¦ç“¶é¢ˆï¼ŒåŠ¡å¿…é€šè¿‡ `max_model_len` è¿›è¡Œå¹³è¡¡ã€‚
2. **å¼•æ“é€‰æ‹©**ï¼šè¿½æ±‚ç¨³å®šæ€§ä¸æ˜“ç”¨æ€§é€‰ **Xinference**ï¼›è¿½æ±‚æé™æ€§èƒ½ä¸åŸç”Ÿ API å…¼å®¹æ€§é€‰ **vLLM**ã€‚
3. **ç½‘ç»œä¼˜åŒ–**ï¼šåœ¨ AutoDL éƒ¨ç½²æ—¶ï¼Œåˆ©ç”¨ `modelscope` ç¯å¢ƒå˜é‡å¯ä»¥èŠ‚çœæ•°å°æ—¶çš„æ¨¡å‹ä¸‹è½½æ—¶é—´ã€‚

å‚è€ƒä¸ªäººåšå®¢é“¾æ¥:
https://blog.csdn.net/weixin_49891405/article/details/157938510?spm=1001.2014.3001.5501

##  Acknowledgments
DataWhale AIå¼€æºç»„ç»‡ (https://github.com/datawhalechina)

